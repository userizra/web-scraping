# Web Crawler Project

A web crawler that fetches, stores, and searches crawled URLs. This project implements a multi-step process of web scraping, crawling, storing data in a database, and providing search functionality through an API. The final product includes a web interface for interacting with crawled data.

## Project Setup

### Tech Stack:
- **Backend**: Python with Flask (or Node.js)
- **Libraries**: 
  - Python: `requests`, `BeautifulSoup`, `Scrapy`
  - Optional: Flask for basic API functionality
- **Frontend (Optional)**: React or Next.js for web interface
- **Database**: SQLite or Firebase for storing crawled URLs and relationships

### Requirements

- Python 3.x
- Node.js (if using the frontend)
- Virtual environment (for Python setup)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/web-crawler.git
   cd web-crawler
